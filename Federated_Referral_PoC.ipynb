{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKL6BLTyssLFc6W19rvX/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GQ131/24-Frames-in-Data-Film-Data-Set-Analysis/blob/main/Federated_Referral_PoC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Federated Referral PoC (Option A) ==================\n",
        "# Rural-only, ZIP→Lat/Long/County join, loop preview, Tableau-ready CSV\n",
        "# Time window: Jan 1–Dec 31, 2024 | Program: RSEP\n",
        "# ======================================================================\n",
        "from google.colab import files\n",
        "import pandas as pd, numpy as np, uuid, re\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "1An-zM9sopVF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "e39c3548-744d-470e-cbe7-5f8f0a382d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-25093f7e-f169-4139-95a2-99ebe9674081\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-25093f7e-f169-4139-95a2-99ebe9674081\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ALRB_cases.xlsx to ALRB_cases (10).xlsx\n",
            "Saving DOSH_cases.xlsx to DOSH_cases (10).xlsx\n",
            "Saving LCO_cases.xlsx to LCO_cases (10).xlsx\n",
            "Saving State_Agencies_Locations.xlsx to State_Agencies_Locations (2).xlsx\n",
            "User uploaded file \"ALRB_cases (10).xlsx\" with length 32512 bytes\n",
            "User uploaded file \"DOSH_cases (10).xlsx\" with length 26364811 bytes\n",
            "User uploaded file \"LCO_cases (10).xlsx\" with length 680909 bytes\n",
            "User uploaded file \"State_Agencies_Locations (2).xlsx\" with length 15319 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Update these paths to your uploaded files in Colab ----\n",
        "ZIP_LATLONG_PATH = \"/content/California_Zip_Lat_Lng_City_State_County.csv\"  # has: Country, State/Province, ZIP Code/Postcode, Latitude, Longitude, County, City\n",
        "DLSE_PATH        = \"/content/LCO_cases.xlsx\"\n",
        "ALRB_PATH        = \"/content/ALRB_cases.xlsx\"\n",
        "CALOSHA_PATH     = \"/content/DOSH_cases.xlsx\"\n"
      ],
      "metadata": {
        "id": "CIdhclxjoE9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('/content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQTEoHj-qVKk",
        "outputId": "0d59de75-a602-4e72-86c8-cfa5b4390097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'DOSH_cases (3).xlsx',\n",
              " 'LCO_cases (10).xlsx',\n",
              " 'California_Zip_Lat_Lng_City_State_County (2).csv',\n",
              " 'LCO_cases (2).xlsx',\n",
              " 'State_Agencies_Locations.xlsx',\n",
              " 'ALRB_cases (2).xlsx',\n",
              " 'California_Zip_Lat_Lng_City_State_County (8).csv',\n",
              " 'California_Zip_Lat_Lng_City_State_County (6).csv',\n",
              " 'California_Zip_Lat_Lng_City_State_County (5).csv',\n",
              " 'California_Zip_Lat_Lng_City_State_County (1).csv',\n",
              " 'ALRB_cases (8).xlsx',\n",
              " 'LCO_cases (7).xlsx',\n",
              " 'ALRB_cases.xlsx',\n",
              " 'DOSH_cases (6).xlsx',\n",
              " 'ALRB_cases (4).xlsx',\n",
              " 'ALRB_cases (6).xlsx',\n",
              " 'DOSH_cases (10).xlsx',\n",
              " 'ALRB_cases (10).xlsx',\n",
              " 'DOSH_cases (5).xlsx',\n",
              " 'DOSH_cases (2).xlsx',\n",
              " 'California_Zip_Lat_Lng_City_State_County (4).csv',\n",
              " 'DOSH_cases (7).xlsx',\n",
              " 'California_Zip_Lat_Lng_City_State_County (7).csv',\n",
              " 'ALRB_cases (7).xlsx',\n",
              " 'ALRB_cases (5).xlsx',\n",
              " 'California_Zip_Lat_Lng_City_State_County (3).csv',\n",
              " 'DOSH_cases (8).xlsx',\n",
              " 'LCO_cases (9).xlsx',\n",
              " 'DOSH_cases (1).xlsx',\n",
              " 'LCO_cases (6).xlsx',\n",
              " 'State_Agencies_Locations (1).xlsx',\n",
              " 'LCO_cases (5).xlsx',\n",
              " 'LCO_cases (8).xlsx',\n",
              " 'DOSH_cases (4).xlsx',\n",
              " 'tableau_referrals_poc_rural_only.csv',\n",
              " 'LCO_cases (3).xlsx',\n",
              " 'LCO_cases (1).xlsx',\n",
              " 'DOSH_cases.xlsx',\n",
              " 'LCO_cases (4).xlsx',\n",
              " 'ALRB_cases (1).xlsx',\n",
              " 'LCO_cases.xlsx',\n",
              " 'DOSH_cases (9).xlsx',\n",
              " 'California_Zip_Lat_Lng_City_State_County.csv',\n",
              " 'ALRB_cases (9).xlsx',\n",
              " 'State_Agencies_Locations (2).xlsx',\n",
              " 'ALRB_cases (3).xlsx',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- PoC knobs ----\n",
        "ALL_AGENCIES = [\"DIR\",\"Cal/OSHA\",\"DLSE\",\"ALRB\",\"EDD\"]\n",
        "PROGRAM_VALUE = \"RSEP\"\n",
        "LOOP_RATE = 0.08                       # ~8% second-hop preview\n",
        "MIN_SEND_LAG, MAX_SEND_LAG = 0, 7      # days intake -> referral\n",
        "MIN_STATUS_LAG, MAX_STATUS_LAG = 3, 21 # days referral -> first update\n",
        "TARGET_MAX_ROWS = 600\n",
        "YEARS = {2023, 2024}"
      ],
      "metadata": {
        "id": "b6Jgkw7ioGnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Canonical output columns (used everywhere to keep shape stable)\n",
        "COLS_CANON = [\n",
        "    \"Referral_UID\",\"Complaint_Intake_Date\",\"Referral_Sent_Date\",\n",
        "    \"Origin_Agency\",\"Destination_Agency\",\"Program\",\n",
        "    \"County\",\"ZIP\",\"Latitude\",\"Longitude\",\"Is_Rural_Worker\",\n",
        "    \"Referral_Status\",\"Response_Completed\",\"Days_To_First_Status_Update\",\n",
        "    \"Loop_Destination_Agency\",\"Loop_Referral_Sent_Date\",\n",
        "]"
      ],
      "metadata": {
        "id": "gNXdBzSanK8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Curated rural counties list (tune if needed)\n",
        "RURAL_COUNTIES = {\n",
        "    \"Amador\",\"Calaveras\",\"Colusa\",\"Del Norte\",\"El Dorado\",\"Glenn\",\"Humboldt\",\"Imperial\",\"Inyo\",\n",
        "    \"Kings\",\"Lake\",\"Lassen\",\"Madera\",\"Mariposa\",\"Mendocino\",\"Merced\",\"Modoc\",\"Mono\",\"Napa\",\n",
        "    \"Nevada\",\"Placer\",\"Plumas\",\"San Benito\",\"Shasta\",\"Sierra\",\"Siskiyou\",\"Sutter\",\"Tehama\",\n",
        "    \"Trinity\",\"Tulare\",\"Tuolumne\",\"Yuba\",\"Kern\", \"Fresno\", \"Stanislaus\"\n",
        "}\n",
        "\n",
        "# ---- Helpers ----\n",
        "def zip5(z):\n",
        "    if pd.isna(z):\n",
        "        return \"\"\n",
        "    s = str(z).strip()\n",
        "    m = re.search(r\"(\\d{5})\", s)\n",
        "    return m.group(1) if m else \"\"\n",
        "\n",
        "def coerce_date(s):\n",
        "    return pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "def normalize_status_from_closed(closed_date, status_hint=None):\n",
        "    if pd.notna(closed_date):\n",
        "        return \"Closed\"\n",
        "    if status_hint is not None and isinstance(status_hint, str):\n",
        "        sl = status_hint.lower()\n",
        "        if any(k in sl for k in [\"close\",\"settle\",\"resolved\",\"dismiss\"]):\n",
        "            return \"Closed\"\n",
        "        if any(k in sl for k in [\"progress\",\"investigat\",\"acknow\"]):\n",
        "            return \"In Progress\"\n",
        "    return \"In Progress\"\n",
        "\n",
        "def choose_destination(origin, weights):\n",
        "    w = dict(weights) if weights else {origin: 0.6}\n",
        "    for a in ALL_AGENCIES:\n",
        "        w.setdefault(a, 0.0)\n",
        "    agencies, probs = zip(*w.items())\n",
        "    probs = np.array(probs, dtype=float)\n",
        "    probs = probs / probs.sum() if probs.sum() else np.ones_like(probs)/len(probs)\n",
        "    return np.random.choice(agencies, p=probs)\n",
        "\n",
        "def choose_destination_diff(origin, weights):\n",
        "    \"\"\"\n",
        "    Pick a destination agency different from origin AND never DIR.\n",
        "    If weights include DIR or origin, they are automatically excluded and renormalized.\n",
        "    \"\"\"\n",
        "    # Start from provided weights\n",
        "    w = dict(weights) if weights else {}\n",
        "\n",
        "    # Force DIR weight = 0 so it is never chosen\n",
        "    w[\"DIR\"] = 0.0\n",
        "\n",
        "    # Also force origin = 0\n",
        "    w[origin] = 0.0\n",
        "\n",
        "    # Ensure all agencies exist in the dictionary\n",
        "    for a in ALL_AGENCIES:\n",
        "        w.setdefault(a, 0.0)\n",
        "\n",
        "    # Candidate agencies exclude DIR and exclude origin\n",
        "    agencies = [a for a in ALL_AGENCIES if a not in (\"DIR\", origin)]\n",
        "    probs = np.array([w[a] for a in agencies], dtype=float)\n",
        "\n",
        "    # Renormalize or fallback uniform if needed\n",
        "    probs = probs / probs.sum() if probs.sum() > 0 else np.ones(len(agencies)) / len(agencies)\n",
        "\n",
        "    return np.random.choice(agencies, p=probs)\n",
        "\n",
        "\n",
        "def pick_status_and_completion(closed_flag):\n",
        "    if closed_flag:\n",
        "        return \"Closed\",\"Y\"\n",
        "    return np.random.choice([\"In Progress\",\"Open\"], p=[0.75,0.25]), np.random.choice([\"Y\",\"N\"], p=[0.5,0.5])\n",
        "\n",
        "def load_xlsx(path, sheet=None, header=None):\n",
        "    if sheet is None:\n",
        "        # read all sheets with default header=0\n",
        "        x = pd.read_excel(path, sheet_name=None)\n",
        "        frames = []\n",
        "        for name, df in x.items():\n",
        "            df = df.copy()\n",
        "            df[\"__sheet__\"] = name\n",
        "            frames.append(df)\n",
        "        return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
        "    # default to header=0 unless a custom header row is provided\n",
        "    return pd.read_excel(path, sheet_name=sheet, header=0 if header is None else header)\n",
        "\n"
      ],
      "metadata": {
        "id": "DP8sk3uUoQGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Load ZIP→Lat/Long/County lookup and normalize ZIP ----\n",
        "geo = pd.read_csv(ZIP_LATLONG_PATH, dtype=str, keep_default_na=False)\n",
        "zip_col = \"ZIP Code/Postcode\"; lat_col = \"Latitude\"; lon_col = \"Longitude\"; county_col = \"County\"\n",
        "geo = geo.rename(columns={zip_col:\"ZIP5\", lat_col:\"Latitude\", lon_col:\"Longitude\", county_col:\"GeoCounty\"})\n",
        "geo[\"ZIP5\"] = geo[\"ZIP5\"].apply(zip5)\n",
        "geo = geo[(geo[\"ZIP5\"]!=\"\") & (geo[\"GeoCounty\"]!=\"\")]\n",
        "geo = geo[[\"ZIP5\",\"Latitude\",\"Longitude\",\"GeoCounty\"]].drop_duplicates()"
      ],
      "metadata": {
        "id": "1JOyjPmpoZ_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Agency configs (based on your columns) ----\n",
        "CONFIG = {\n",
        "    \"DLSE\": {\n",
        "        \"path\": DLSE_PATH, \"origin\": \"DLSE\", \"sheet\": 0,\n",
        "        \"cols\": {\n",
        "            \"intake_date\": \"Case Management: Created Date\",\n",
        "            \"closed_date\": \"Case Closed Date\",\n",
        "            \"county\": None,\n",
        "            \"zip\": \"Employer: Primary Zip/Postal Code\",\n",
        "            \"status_hint\": None,\n",
        "            \"case_id\": \"Case Management: DIR Case Name\",\n",
        "        },\n",
        "        \"routing_weights\": {\"DLSE\": 0.60, \"Cal/OSHA\": 0.20, \"DIR\": 0.10, \"ALRB\": 0.05, \"EDD\": 0.05},\n",
        "    },\n",
        "    \"ALRB\": {\n",
        "        \"path\": ALRB_PATH, \"origin\": \"ALRB\", \"sheet\": None,\n",
        "        \"cols\": {\n",
        "            \"intake_date\": \"Filed Date\",\n",
        "            \"closed_date\": None,\n",
        "            \"county\": \"County\",\n",
        "            \"zip\": None,\n",
        "            \"status_hint\": \"Outcome\",\n",
        "            \"case_id\": \"Case Number\",\n",
        "        },\n",
        "        \"routing_weights\": {\"ALRB\": 0.65, \"DLSE\": 0.10, \"Cal/OSHA\": 0.10, \"DIR\": 0.10, \"EDD\": 0.05},\n",
        "    },\n",
        "\n",
        "    # In your CONFIG block for Cal/OSHA:\n",
        "    \"Cal/OSHA\": {\n",
        "        \"path\": CALOSHA_PATH, \"origin\": \"Cal/OSHA\",\n",
        "        \"sheet\": \"Complaints\",\n",
        "        \"header_row\": 14,\n",
        "        \"cols\": {\n",
        "            \"intake_date\": \"Open Conf Date\",\n",
        "            \"closed_date\": \"Closed Date\",\n",
        "            \"county\": \"GeoCounty\",   # <- use GeoCounty\n",
        "            \"zip\": \"GeoZip\",         # <- use GeoZip\n",
        "            \"status_hint\": None,\n",
        "            \"case_id\": \"RID\",\n",
        "        },\n",
        "        \"routing_weights\": {\"Cal/OSHA\": 0.50, \"DLSE\": 0.30, \"DIR\": 0.10, \"ALRB\": 0.05, \"EDD\": 0.05},\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "-MCx_r_TodwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx9pla3Pn7_I"
      },
      "outputs": [],
      "source": [
        "# ---- Builder per agency (handles blanks, uneven sizes, 2024 filter) ----\n",
        "\n",
        "def empty_out():\n",
        "    \"\"\"Return an empty dataframe with canonical columns.\"\"\"\n",
        "    return pd.DataFrame({c: pd.Series(dtype=\"object\") for c in COLS_CANON})[COLS_CANON]\n",
        "\n",
        "def build_from_agency(cfg):\n",
        "    df_raw = load_xlsx(cfg[\"path\"], cfg.get(\"sheet\"), cfg.get(\"header_row\"))\n",
        "    if df_raw is None or df_raw.empty:\n",
        "        return empty_out()\n",
        "\n",
        "    c = cfg[\"cols\"]; ori = cfg[\"origin\"]\n",
        "\n",
        "    # graceful fallbacks\n",
        "    if c.get(\"county\") and c[\"county\"] not in df_raw.columns and \"County\" in df_raw.columns:\n",
        "        c[\"county\"] = \"County\"\n",
        "    if c.get(\"zip\") and c[\"zip\"] not in df_raw.columns:\n",
        "        if \"Site  Zip\" in df_raw.columns:\n",
        "            c[\"zip\"] = \"Site  Zip\"\n",
        "\n",
        "    # Required intake date column must exist\n",
        "    if not c.get(\"intake_date\") or c[\"intake_date\"] not in df_raw.columns:\n",
        "        return empty_out()\n",
        "\n",
        "    intake = coerce_date(df_raw[c[\"intake_date\"]])\n",
        "    closed = coerce_date(df_raw[c[\"closed_date\"]]) if c.get(\"closed_date\") and c[\"closed_date\"] in df_raw.columns else pd.Series(pd.NaT, index=df_raw.index)\n",
        "    status_hint = df_raw[c[\"status_hint\"]] if c.get(\"status_hint\") and c[\"status_hint\"] in df_raw.columns else None\n",
        "\n",
        "    # Years\n",
        "    mask_years = intake.notna() & (intake.dt.year.isin(YEARS))\n",
        "    if not mask_years.any():\n",
        "        return empty_out()\n",
        "\n",
        "    df = df_raw.loc[mask_years].copy()\n",
        "    intake = intake.loc[mask_years]\n",
        "    closed = closed.loc[mask_years] if isinstance(closed, pd.Series) else pd.Series(pd.NaT, index=df.index)\n",
        "    if isinstance(status_hint, pd.Series):\n",
        "        status_hint = status_hint.loc[mask_years]\n",
        "\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out[\"Referral_UID\"] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
        "    out[\"Complaint_Intake_Date\"] = intake.dt.date\n",
        "\n",
        "    # Referral sent date\n",
        "    send_lag = np.random.randint(MIN_SEND_LAG, MAX_SEND_LAG+1, size=len(df))\n",
        "    ref_sent = (intake + pd.to_timedelta(send_lag, unit=\"D\"))\n",
        "    out[\"Referral_Sent_Date\"] = ref_sent.dt.date\n",
        "    out[\"Origin_Agency\"] = ori\n",
        "    out[\"Program\"] = PROGRAM_VALUE\n",
        "\n",
        "    # ZIP normalized → 5-digit\n",
        "    if c.get(\"zip\") and c[\"zip\"] in df.columns:\n",
        "        out[\"ZIP\"] = df[c[\"zip\"]].apply(zip5)\n",
        "    else:\n",
        "        out[\"ZIP\"] = \"\"\n",
        "\n",
        "    # County: direct if present, else infer from ZIP via geo\n",
        "    if c.get(\"county\") and c[\"county\"] in df.columns:\n",
        "        county_series = df[c[\"county\"]].fillna(\"\").astype(str).str.strip()\n",
        "    else:\n",
        "        county_series = pd.Series([\"\"]*len(df), index=df.index)\n",
        "\n",
        "    # Merge ZIP→(County, Lat, Lon)\n",
        "    z = out[[\"ZIP\"]].copy().rename(columns={\"ZIP\":\"ZIP5\"})\n",
        "    merged = z.merge(geo, how=\"left\", on=\"ZIP5\")\n",
        "\n",
        "    # *** Align merged to out's index to avoid shape mismatches ***\n",
        "    merged.index = out.index\n",
        "\n",
        "    # Lat/Lon from ZIP master (will be blank if ZIP missing/not found)\n",
        "    out[\"Latitude\"]  = merged[\"Latitude\"].fillna(\"\")\n",
        "    out[\"Longitude\"] = merged[\"Longitude\"].fillna(\"\")\n",
        "\n",
        "    # ----- County resolution (Policy: B and 1) -----\n",
        "    # Prefer County from ZIP master when ZIP is present; otherwise keep agency County.\n",
        "    county_from_agency = (\n",
        "        county_series.reindex(out.index)   # <-- align to out\n",
        "                    .fillna(\"\")\n",
        "                    .astype(str).str.strip()\n",
        "    )\n",
        "    county_from_zip = (\n",
        "        merged[\"GeoCounty\"].fillna(\"\")\n",
        "                          .astype(str).str.strip()\n",
        "    )\n",
        "\n",
        "    out[\"County\"] = np.where(\n",
        "        (out[\"ZIP\"] != \"\") & (county_from_zip != \"\"),\n",
        "        county_from_zip,\n",
        "        county_from_agency\n",
        "    )\n",
        "\n",
        "    # Rural rule\n",
        "    out[\"Is_Rural_Worker\"] = out[\"County\"].apply(lambda cn: \"Y\" if cn in RURAL_COUNTIES else \"N\")\n",
        "\n",
        "    # Jurisdiction filter: rural only\n",
        "    out = out[out[\"Is_Rural_Worker\"]==\"Y\"].copy()\n",
        "    if out.empty:\n",
        "        return empty_out()\n",
        "\n",
        "    # Destination selection\n",
        "    weights = cfg.get(\"routing_weights\", {})\n",
        "    out[\"Destination_Agency\"] = [choose_destination_diff(ori, weights) for _ in range(len(out))]\n",
        "\n",
        "    # Status & completion\n",
        "    closed_map = pd.Series(closed.values, index=df.index).loc[out.index]\n",
        "    if isinstance(status_hint, pd.Series):\n",
        "        status_hint_map = pd.Series(status_hint.values, index=df.index).loc[out.index]\n",
        "    else:\n",
        "        status_hint_map = pd.Series([None]*len(out), index=out.index)\n",
        "\n",
        "    norm_status = [\n",
        "        normalize_status_from_closed(closed_map.loc[i], status_hint_map.loc[i])\n",
        "        for i in out.index\n",
        "    ]\n",
        "    is_closed = [s == \"Closed\" for s in norm_status]\n",
        "    final_status, completed = [], []\n",
        "    for cl in is_closed:\n",
        "        s, comp = pick_status_and_completion(cl)\n",
        "        if cl: s, comp = \"Closed\",\"Y\"\n",
        "        final_status.append(s); completed.append(comp)\n",
        "    out[\"Referral_Status\"] = final_status\n",
        "    out[\"Response_Completed\"] = completed\n",
        "\n",
        "    # Days to first update\n",
        "    out[\"Days_To_First_Status_Update\"] = np.random.randint(MIN_STATUS_LAG, MAX_STATUS_LAG+1, size=len(out))\n",
        "\n",
        "    # Loop preview (~8%)\n",
        "    n = len(out)\n",
        "    mask_loop = np.random.rand(n) < LOOP_RATE\n",
        "    loop_dest, loop_date = [], []\n",
        "    for idx, m in zip(out.index, mask_loop):\n",
        "        if not m:\n",
        "            loop_dest.append(\"\"); loop_date.append(\"\")\n",
        "            continue\n",
        "        first_dest = out.loc[idx, \"Destination_Agency\"]\n",
        "        origin_ag  = out.loc[idx, \"Origin_Agency\"]\n",
        "\n",
        "        # exclude DIR, origin, and first hop\n",
        "        choices = [a for a in ALL_AGENCIES if a not in (\"DIR\", origin_ag, first_dest)]\n",
        "\n",
        "        # safety fallback (should rarely trigger)\n",
        "        if not choices:\n",
        "            choices = [a for a in ALL_AGENCIES if a not in (\"DIR\", first_dest)]\n",
        "\n",
        "        loop_dest.append(np.random.choice(choices))\n",
        "\n",
        "        d0 = pd.to_datetime(out.loc[idx, \"Referral_Sent_Date\"])\n",
        "        loop_date.append((d0 + timedelta(days=int(np.random.randint(2,11)))).date())\n",
        "    out[\"Loop_Destination_Agency\"] = loop_dest\n",
        "    out[\"Loop_Referral_Sent_Date\"] = loop_date\n",
        "\n",
        "    # Ensure canonical order\n",
        "    for ccc in COLS_CANON:\n",
        "        if ccc not in out.columns:\n",
        "            out[ccc] = \"\"\n",
        "    return out[COLS_CANON]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Build all and export ------------------\n",
        "frames = []\n",
        "for key, cfg in CONFIG.items():\n",
        "    print(f\"Processing: {key}\")\n",
        "    frames.append(build_from_agency(cfg))\n",
        "\n",
        "# If everything came back empty, still produce a CSV with headers\n",
        "if len(frames) == 0:\n",
        "    final = empty_out()\n",
        "else:\n",
        "    final = pd.concat(frames, ignore_index=True) if any(not f.empty for f in frames) else empty_out()\n",
        "\n",
        "# Cap size for PoC if very large; sort by date for nicer Tableau behavior\n",
        "if len(final) > TARGET_MAX_ROWS:\n",
        "    final = final.sample(TARGET_MAX_ROWS, random_state=42).sort_values(\"Referral_Sent_Date\")\n",
        "\n",
        "    self_refs = (final[\"Origin_Agency\"] == final[\"Destination_Agency\"]).sum()\n",
        "print(\"Self-referrals (should be 0):\", (final[\"Origin_Agency\"] == final[\"Destination_Agency\"]).sum())\n",
        "print(\"Any DIR as Destination (should be 0):\", (final[\"Destination_Agency\"] == \"DIR\").sum())\n",
        "print(\"Any DIR in Loop_Destination_Agency (should be 0):\", (final[\"Loop_Destination_Agency\"] == \"DIR\").sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVISUEzvVDmP",
        "outputId": "e5835b61-1212-4736-c7b2-735b5ec57e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: DLSE\n",
            "Processing: ALRB\n",
            "Processing: Cal/OSHA\n",
            "Self-referrals (should be 0): 0\n",
            "Any DIR as Destination (should be 0): 0\n",
            "Any DIR in Loop_Destination_Agency (should be 0): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Attach nearest Destination office (Agency_ID/City/ZIP) ----------\n",
        "AGENCY_OFFICES_XLSX = \"/content/State_Agencies_Locations.xlsx\"  # adjust if needed\n",
        "off = pd.read_excel(AGENCY_OFFICES_XLSX)\n",
        "\n",
        "# Keep only what we need and normalize column names\n",
        "off = off.rename(columns={\n",
        "    \"Agency_ID\": \"Agency_ID\",\n",
        "    \"affiliated_agency\": \"affiliated_agency\",\n",
        "    \"city\": \"Office_City\",\n",
        "    \"zipcode\": \"Office_ZIP\",\n",
        "    \"Latitude\": \"Office_Lat\",\n",
        "    \"Longitude\": \"Office_Lon\",\n",
        "})\n",
        "off[\"Office_Lat\"] = pd.to_numeric(off[\"Office_Lat\"], errors=\"coerce\")\n",
        "off[\"Office_Lon\"] = pd.to_numeric(off[\"Office_Lon\"], errors=\"coerce\")\n",
        "off = off.dropna(subset=[\"Office_Lat\",\"Office_Lon\"])\n",
        "\n",
        "# Collapse CalOsha variants to one label; ignore DWC\n",
        "def normalize_locations_agency(a: str) -> str:\n",
        "    if not isinstance(a, str): return \"\"\n",
        "    s = a.strip()\n",
        "    low = s.lower()\n",
        "    if low.startswith(\"calosha\"): return \"CalOsha\"  # CalOsha, CalOsha (LETF), etc.\n",
        "    if low == \"lco\":            return \"LCO\"\n",
        "    if low == \"alrb\":           return \"ALRB\"\n",
        "    if low == \"edd\":            return \"EDD\"\n",
        "    if low == \"dwc\":            return \"\"           # ignore\n",
        "    return s\n",
        "\n",
        "off[\"AgencyBase\"] = off[\"affiliated_agency\"].apply(normalize_locations_agency)\n",
        "off = off[off[\"AgencyBase\"].isin([\"CalOsha\",\"LCO\",\"ALRB\",\"EDD\"])].copy()\n",
        "\n",
        "# Map referral Destination_Agency -> locations AgencyBase\n",
        "def referral_to_base(a: str) -> str:\n",
        "    if not isinstance(a, str): return \"\"\n",
        "    s = a.strip().lower()\n",
        "    if s in (\"cal/osha\",\"cal osha\",\"calosha\",\"cal-osha\"): return \"CalOsha\"\n",
        "    if s == \"dlse\":  return \"LCO\"\n",
        "    if s == \"alrb\":  return \"ALRB\"\n",
        "    if s == \"edd\":   return \"EDD\"\n",
        "    # DIR has no offices in your table; return empty so we leave blanks\n",
        "    return \"\"\n",
        "\n",
        "final[\"_Dest_Base\"] = final[\"Destination_Agency\"].apply(referral_to_base)\n",
        "\n",
        "# Prepare worker coordinates (from ZIP geocode) as numeric for distance calc\n",
        "final[\"_Worker_Lat\"] = pd.to_numeric(final[\"Latitude\"], errors=\"coerce\")\n",
        "final[\"_Worker_Lon\"] = pd.to_numeric(final[\"Longitude\"], errors=\"coerce\")\n",
        "\n",
        "# Initialize destination office fields\n",
        "for c in [\"Destination_Agency_ID\",\"Destination_Office_City\",\"Destination_Office_ZIP\"]:\n",
        "    if c not in final.columns:\n",
        "        final[c] = \"\"\n",
        "\n",
        "# Haversine (km), vectorized\n",
        "import numpy as np\n",
        "def haversine_km(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.009\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1[:, None]\n",
        "    dlon = lon2 - lon1[:, None]\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)[:,None]*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
        "    return 2*R*np.arcsin(np.sqrt(a))\n",
        "\n",
        "# Attach nearest office per Destination\n",
        "for base in [\"CalOsha\",\"LCO\",\"ALRB\",\"EDD\"]:\n",
        "    mask = (final[\"_Dest_Base\"] == base) & final[\"_Worker_Lat\"].notna() & final[\"_Worker_Lon\"].notna()\n",
        "    if not mask.any():\n",
        "        continue\n",
        "\n",
        "    offices = off[off[\"AgencyBase\"] == base][[\"Agency_ID\",\"Office_Lat\",\"Office_Lon\",\"Office_City\",\"Office_ZIP\"]]\n",
        "    if offices.empty:\n",
        "        continue\n",
        "\n",
        "    worker_pts = final.loc[mask, [\"_Worker_Lat\",\"_Worker_Lon\"]].to_numpy()\n",
        "    office_pts = offices[[\"Office_Lat\",\"Office_Lon\"]].to_numpy()\n",
        "\n",
        "    D = haversine_km(worker_pts[:,0], worker_pts[:,1], office_pts[:,0], office_pts[:,1])\n",
        "    nearest_idx = D.argmin(axis=1)\n",
        "    chosen = offices.reset_index(drop=True).iloc[nearest_idx]\n",
        "\n",
        "    idx = final.index[mask]\n",
        "    final.loc[idx, \"Destination_Agency_ID\"]   = chosen[\"Agency_ID\"].astype(str).values\n",
        "    final.loc[idx, \"Destination_Office_City\"] = chosen[\"Office_City\"].astype(str).values\n",
        "    final.loc[idx, \"Destination_Office_ZIP\"]  = chosen[\"Office_ZIP\"].astype(str).values\n",
        "\n",
        "# Optional: privacy cleanup — remove temps or worker coords entirely\n",
        "final = final.drop(columns=[\"_Worker_Lat\",\"_Worker_Lon\",\"_Dest_Base\"], errors=\"ignore\")\n",
        "# If you want to exclude worker lat/lon from the export:\n",
        "# final = final.drop(columns=[\"Latitude\",\"Longitude\",\"ZIP\"], errors=\"ignore\")\n",
        "\n",
        "# Quick QA (optional)\n",
        "# print(\"Destination blanks (Agency_ID):\", (final[\"Destination_Agency_ID\"]==\"\").sum())\n"
      ],
      "metadata": {
        "id": "5OUDR0V-ojaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Attach nearest ORIGIN office (Origin_Agency_ID / City / ZIP) ----------\n",
        "\n",
        "# Reuse office table if present; otherwise load & prep it (same normalization as Destination block)\n",
        "if \"off\" not in locals():\n",
        "    AGENCY_OFFICES_XLSX = \"/content/State_Agencies_Locations.xlsx\"  # adjust if needed\n",
        "    off = pd.read_excel(AGENCY_OFFICES_XLSX).rename(columns={\n",
        "        \"Agency_ID\": \"Agency_ID\",\n",
        "        \"affiliated_agency\": \"affiliated_agency\",\n",
        "        \"city\": \"Office_City\",\n",
        "        \"zipcode\": \"Office_ZIP\",\n",
        "        \"Latitude\": \"Office_Lat\",\n",
        "        \"Longitude\": \"Office_Lon\",\n",
        "    })\n",
        "    off[\"Office_Lat\"] = pd.to_numeric(off[\"Office_Lat\"], errors=\"coerce\")\n",
        "    off[\"Office_Lon\"] = pd.to_numeric(off[\"Office_Lon\"], errors=\"coerce\")\n",
        "    off = off.dropna(subset=[\"Office_Lat\",\"Office_Lon\"])\n",
        "\n",
        "    def _normalize_locations_agency(a: str) -> str:\n",
        "        if not isinstance(a, str): return \"\"\n",
        "        s = a.strip()\n",
        "        low = s.lower()\n",
        "        if low.startswith(\"calosha\"): return \"CalOsha\"  # CalOsha, CalOsha (LETF), etc.\n",
        "        if low == \"lco\":            return \"LCO\"\n",
        "        if low == \"alrb\":           return \"ALRB\"\n",
        "        if low == \"edd\":            return \"EDD\"\n",
        "        if low == \"dwc\":            return \"\"           # ignore\n",
        "        return s\n",
        "    off[\"AgencyBase\"] = off[\"affiliated_agency\"].apply(_normalize_locations_agency)\n",
        "    off = off[off[\"AgencyBase\"].isin([\"CalOsha\",\"LCO\",\"ALRB\",\"EDD\"])].copy()\n",
        "\n",
        "# Reuse haversine if present; otherwise define it\n",
        "import numpy as np\n",
        "if \"haversine_km\" not in locals():\n",
        "    def haversine_km(lat1, lon1, lat2, lon2):\n",
        "        R = 6371.009\n",
        "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "        dlat = lat2 - lat1[:, None]\n",
        "        dlon = lon2 - lon1[:, None]\n",
        "        a = np.sin(dlat/2.0)**2 + np.cos(lat1)[:,None]*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
        "        return 2*R*np.arcsin(np.sqrt(a))\n",
        "\n",
        "# Map Origin_Agency -> locations AgencyBase (DLSE -> LCO, etc.)\n",
        "def origin_to_base(a: str) -> str:\n",
        "    if not isinstance(a, str): return \"\"\n",
        "    s = a.strip().lower()\n",
        "    if s in (\"cal/osha\",\"cal osha\",\"calosha\",\"cal-osha\"): return \"CalOsha\"\n",
        "    if s == \"dlse\":  return \"LCO\"   # YES: DLSE maps to LCO offices\n",
        "    if s == \"alrb\":  return \"ALRB\"\n",
        "    if s == \"edd\":   return \"EDD\"\n",
        "    # DIR has no offices in the locations file → return empty, leave blanks\n",
        "    return \"\"\n",
        "\n",
        "final[\"_Origin_Base\"] = final[\"Origin_Agency\"].apply(origin_to_base)\n",
        "\n",
        "# Worker coords for distance calc (temporary)\n",
        "final[\"_Worker_Lat\"] = pd.to_numeric(final.get(\"Latitude\", \"\"), errors=\"coerce\")\n",
        "final[\"_Worker_Lon\"] = pd.to_numeric(final.get(\"Longitude\", \"\"), errors=\"coerce\")\n",
        "\n",
        "# Initialize origin office fields if missing\n",
        "for c in [\"Origin_Agency_ID\",\"Origin_Office_City\",\"Origin_Office_ZIP\"]:\n",
        "    if c not in final.columns:\n",
        "        final[c] = \"\"\n",
        "\n",
        "# Attach nearest office per Origin base\n",
        "for base in [\"CalOsha\",\"LCO\",\"ALRB\",\"EDD\"]:\n",
        "    mask = (final[\"_Origin_Base\"] == base) & final[\"_Worker_Lat\"].notna() & final[\"_Worker_Lon\"].notna()\n",
        "    if not mask.any():\n",
        "        continue\n",
        "\n",
        "    offices = off[off[\"AgencyBase\"] == base][[\"Agency_ID\",\"Office_Lat\",\"Office_Lon\",\"Office_City\",\"Office_ZIP\"]]\n",
        "    if offices.empty:\n",
        "        continue\n",
        "\n",
        "    worker_pts = final.loc[mask, [\"_Worker_Lat\",\"_Worker_Lon\"]].to_numpy()\n",
        "    office_pts  = offices[[\"Office_Lat\",\"Office_Lon\"]].to_numpy()\n",
        "\n",
        "    D = haversine_km(worker_pts[:,0], worker_pts[:,1], office_pts[:,0], office_pts[:,1])\n",
        "    nearest_idx = D.argmin(axis=1)\n",
        "    chosen = offices.reset_index(drop=True).iloc[nearest_idx]\n",
        "\n",
        "    idx = final.index[mask]\n",
        "    final.loc[idx, \"Origin_Agency_ID\"]   = chosen[\"Agency_ID\"].astype(str).values\n",
        "    final.loc[idx, \"Origin_Office_City\"] = chosen[\"Office_City\"].astype(str).values\n",
        "    final.loc[idx, \"Origin_Office_ZIP\"]  = chosen[\"Office_ZIP\"].astype(str).values\n",
        "\n",
        "# Clean temp columns\n",
        "final = final.drop(columns=[\"_Worker_Lat\",\"_Worker_Lon\",\"_Origin_Base\"], errors=\"ignore\")\n",
        "\n",
        "# (Optional) Quick QA:\n",
        "# print(\"Origin blanks (Agency_ID) — expected mostly DIR origins:\", (final[\"Origin_Agency_ID\"]==\"\").sum())\n"
      ],
      "metadata": {
        "id": "RnNZfPlVlr8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL TYPE HYGIENE BEFORE EXPORT ---\n",
        "\n",
        "# ZIP as text\n",
        "final[\"ZIP\"] = final[\"ZIP\"].astype(str)\n",
        "\n",
        "# Lat/Lon as float when possible, else blank string\n",
        "for col in [\"Latitude\",\"Longitude\"]:\n",
        "    final[col] = pd.to_numeric(final[col], errors=\"coerce\")\n",
        "    final[col] = final[col].apply(lambda x: \"\" if pd.isna(x) else x)\n",
        "\n",
        "# Dates to ISO strings\n",
        "for col in [\"Complaint_Intake_Date\",\"Referral_Sent_Date\",\"Loop_Referral_Sent_Date\"]:\n",
        "    final[col] = pd.to_datetime(final[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "OUT_PATH = \"/content/tableau_referrals_poc_rural_only.csv\"\n",
        "final.to_csv(OUT_PATH, index=False)\n",
        "print(f\"\\n✅ Wrote {OUT_PATH} with {len(final)} rural rows.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xahKJYvHVBIY",
        "outputId": "4220e702-62b0-4f19-ed75-acbac8a25871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Wrote /content/tableau_referrals_poc_rural_only.csv with 600 rural rows.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== (A) Preview Summary ==========================\n",
        "# Month & Origin counts, completion rate, avg days, loops count\n",
        "def month(d):\n",
        "    try:\n",
        "        return pd.to_datetime(d).strftime(\"%Y-%m\")\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "summary = final.copy()\n",
        "summary[\"Month\"] = summary[\"Referral_Sent_Date\"].apply(month)\n",
        "summary[\"Loop_Flag\"] = summary[\"Loop_Destination_Agency\"].apply(lambda x: 1 if isinstance(x, str) and x.strip() != \"\" else 0)\n",
        "\n",
        "grouped = summary.groupby([\"Month\",\"Origin_Agency\"], dropna=False).agg(\n",
        "    referrals=(\"Referral_UID\",\"count\"),\n",
        "    completed_pct=(\"Response_Completed\", lambda s: (s.eq(\"Y\").mean()*100).round(1) if len(s) else 0),\n",
        "    avg_days_to_update=(\"Days_To_First_Status_Update\",\"mean\"),\n",
        "    loops=(\"Loop_Flag\",\"sum\")\n",
        ").reset_index()\n",
        "\n",
        "print(\"=== Preview Summary (by Month & Origin_Agency) ===\")\n",
        "print(grouped.sort_values([\"Month\",\"Origin_Agency\"]).to_string(index=False))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S94iuuIhi3Vy",
        "outputId": "592eac4c-896f-4dc5-bf21-dae30c204a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Preview Summary (by Month & Origin_Agency) ===\n",
            "  Month Origin_Agency  referrals  completed_pct avg_days_to_update  loops\n",
            "2023-01          DLSE         13           84.6          12.769231      3\n",
            "2023-02          DLSE         14           85.7          12.571429      1\n",
            "2023-03          ALRB          2            0.0               14.5      0\n",
            "2023-03          DLSE         20           85.0              10.05      2\n",
            "2023-04          DLSE         14          100.0          12.357143      0\n",
            "2023-05          ALRB          2          100.0                8.0      0\n",
            "2023-05          DLSE         14           71.4          11.285714      2\n",
            "2023-06          ALRB          2          100.0                9.5      0\n",
            "2023-06          DLSE         14           85.7               14.5      0\n",
            "2023-07          ALRB          4           25.0               15.5      0\n",
            "2023-07          DLSE         27           88.9          12.259259      3\n",
            "2023-08          ALRB          1            0.0                3.0      0\n",
            "2023-08          DLSE         16           87.5              11.25      3\n",
            "2023-09          ALRB          1            0.0               21.0      0\n",
            "2023-09          DLSE         20           80.0              12.25      0\n",
            "2023-10          DLSE         24           70.8          12.208333      1\n",
            "2023-11          DLSE         33           72.7          11.818182      3\n",
            "2023-12          DLSE         22           86.4          12.954545      2\n",
            "2024-01          ALRB          1          100.0                9.0      0\n",
            "2024-01          DLSE         15           60.0          11.133333      0\n",
            "2024-02          DLSE         20           75.0               11.4      4\n",
            "2024-03          ALRB          4           25.0               13.5      0\n",
            "2024-03          DLSE         17           88.2          13.058824      2\n",
            "2024-04          ALRB          4           25.0               8.75      1\n",
            "2024-04          DLSE         36           77.8          11.833333      2\n",
            "2024-05          DLSE         14           78.6          13.428571      1\n",
            "2024-06          ALRB          4           50.0                9.0      0\n",
            "2024-06          DLSE         11           72.7          14.909091      1\n",
            "2024-07          ALRB          2           50.0                7.0      1\n",
            "2024-07          DLSE         44           84.1          10.340909      2\n",
            "2024-08          ALRB          4           75.0               8.75      1\n",
            "2024-08          DLSE         47           72.3          12.489362      3\n",
            "2024-09          DLSE         38           71.1          12.263158      0\n",
            "2024-10          DLSE         39           56.4          12.128205      2\n",
            "2024-11          DLSE         33           60.6          10.909091      3\n",
            "2024-12          DLSE         22           54.5          10.409091      0\n",
            "2025-01          DLSE          2          100.0               14.5      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Overall quick KPIs\n",
        "overall = {\n",
        "    \"Total referrals\": len(final),\n",
        "    \"Completion rate (%)\": round(final[\"Response_Completed\"].eq(\"Y\").mean()*100, 1) if len(final) else 0,\n",
        "    \"Avg days to first update\": round(final[\"Days_To_First_Status_Update\"].mean(), 1) if len(final) else 0,\n",
        "    \"Loop share (%)\": round((summary[\"Loop_Flag\"].mean()*100), 1) if len(final) else 0,\n",
        "}\n",
        "print(\"\\n=== Overall KPIs ===\")\n",
        "for k,v in overall.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# ====================== (C) Sample Validation ========================\n",
        "print(\"\\n=== Sample 10 rows ===\")\n",
        "print(final.head(10).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4jWTSKVi5bY",
        "outputId": "c35eb52c-684e-4146-eba5-c9490e7033f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Overall KPIs ===\n",
            "Total referrals: 600\n",
            "Completion rate (%): 74.0\n",
            "Avg days to first update: 11.9\n",
            "Loop share (%): 7.3\n",
            "\n",
            "=== Sample 10 rows ===\n",
            "                        Referral_UID Complaint_Intake_Date Referral_Sent_Date Origin_Agency Destination_Agency Program     County   ZIP  Latitude  Longitude Is_Rural_Worker Referral_Status Response_Completed Days_To_First_Status_Update Loop_Destination_Agency Loop_Referral_Sent_Date Destination_Agency_ID Destination_Office_City Destination_Office_ZIP Origin_Agency_ID Origin_Office_City Origin_Office_ZIP\n",
            "8d134f50-f823-42cd-866e-e1d7ac210ec4            2023-01-06         2023-01-08          DLSE                EDD    RSEP       Lake 95467  38.80453 -122.54948               Y            Open                  Y                          18                                             NaN                 ED507              Marysville                  95901            LC112         Santa Rosa             95404\n",
            "3a174b8d-f018-40c0-bb54-2a299da1f1c2            2023-01-09         2023-01-10          DLSE                EDD    RSEP      Kings 93245  36.28614 -119.83405               Y          Closed                  Y                          18                                             NaN                 ED501                 Visalia                  93277            LC105             Fresno             93710\n",
            "7b3791f6-48f6-4c48-8e1a-2454bc3400be            2023-01-13         2023-01-15          DLSE           Cal/OSHA    RSEP       Kern 93263  35.48722 -119.27835               Y          Closed                  Y                          17                                             NaN                 DO235             Bakersfield                  93308            LC101        Bakersfield             93308\n",
            "589be0e9-1fcb-429c-9ed6-3dbb3ffaf0a1            2023-01-09         2023-01-16          DLSE           Cal/OSHA    RSEP      Kings 93245  36.28614 -119.83405               Y          Closed                  Y                          14                                             NaN                 DO226                  Fresno                  93721            LC105             Fresno             93710\n",
            "8728093c-b5be-4915-bac4-5bf4a2b3514e            2023-01-17         2023-01-18          DLSE           Cal/OSHA    RSEP     Merced 95324  37.39452 -120.89605               Y     In Progress                  N                           9                                             NaN                 DO225                 Modesto                  95356            LC114           Stockton             95202\n",
            "d885ae64-6fd6-4565-b0f7-65ff14724f71            2023-01-18         2023-01-20          DLSE           Cal/OSHA    RSEP     Madera 93638   37.0424 -120.03816               Y          Closed                  Y                          15                                             NaN                 DO226                  Fresno                  93721            LC105             Fresno             93710\n",
            "53c20143-8be7-443b-ab1b-2ac62ca463fe            2023-01-23         2023-01-23          DLSE           Cal/OSHA    RSEP      Kings 93245  36.28614 -119.83405               Y          Closed                  Y                          12                     EDD              2023-01-25                 DO226                  Fresno                  93721            LC105             Fresno             93710\n",
            "e1753c58-3a2c-4067-b3ac-368754d17c1e            2023-01-27         2023-01-28          DLSE           Cal/OSHA    RSEP Stanislaus 95351   37.6236 -120.99828               Y     In Progress                  N                          18                                             NaN                 DO225                 Modesto                  95356            LC114           Stockton             95202\n",
            "18e988a1-a2c8-497a-b6b7-82a50d92992b            2023-01-23         2023-01-28          DLSE           Cal/OSHA    RSEP      Kings 93245  36.28614 -119.83405               Y          Closed                  Y                          16                     EDD              2023-01-31                 DO226                  Fresno                  93721            LC105             Fresno             93710\n",
            "1d1067eb-0a4a-4c60-8131-07d7c4a00568            2023-01-27         2023-01-29          DLSE           Cal/OSHA    RSEP     Merced 95369  37.52435 -120.41301               Y          Closed                  Y                           9                                             NaN                 DO225                 Modesto                  95356            LC114           Stockton             95202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_PATH = \"/content/tableau_referrals_poc_rural_only.csv\"\n",
        "final.to_csv(OUT_PATH, index=False)\n",
        "print(f\"\\n✅ Wrote {OUT_PATH} with {len(final)} rural rows.\\n\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(OUT_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "T4Ee2PHmtTkB",
        "outputId": "3ad61e75-839e-4f58-fa43-0edeb9d9d0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Wrote /content/tableau_referrals_poc_rural_only.csv with 600 rural rows.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cfebb6ed-2980-4494-aea9-da4f1a3d6cf5\", \"tableau_referrals_poc_rural_only.csv\", 99997)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List sheets\n",
        "xls = pd.ExcelFile(CALOSHA_PATH)\n",
        "print(\"Sheets:\", xls.sheet_names)\n",
        "\n",
        "# Peek columns from the sheet you're using (adjust index if needed)\n",
        "df_test = pd.read_excel(CALOSHA_PATH, sheet_name=0)\n",
        "print(\"Sheet[0] columns:\", list(df_test.columns))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvYx8p4I5E7c",
        "outputId": "76659252-27b7-4303-84f7-5b886ab01141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sheets: ['Notes', 'References - Lists', 'Sheet1', 'Sheet3', 'Complaints', 'Industries- Tables', 'Industries - Charts', 'All Regions - Charts', 'Complaint vs Insp Coded', 'Heat Referrals', 'Comparison by NAICS', 'Comparison by YR', 'Other Inspections Coded', 'Totals by Ag Subsector', 'Sheet2', 'Region 1 - Charts', 'Region 2 - Charts', 'Region 3 - Charts', 'Region 4 - Charts', 'All Regions - Charts (2)']\n",
            "Sheet[0] columns: ['UPA One Liner Detail']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "xls = pd.ExcelFile(CALOSHA_PATH)\n",
        "\n",
        "def find_col(df, targets):\n",
        "    norm = {c: c.strip().lower().replace(\".\", \"\").replace(\"_\", \"\").replace(\"  \", \" \")\n",
        "            for c in df.columns}\n",
        "    for t in targets:\n",
        "        tnorm = t.strip().lower().replace(\".\", \"\").replace(\"_\", \"\").replace(\"  \", \" \")\n",
        "        for c, cnorm in norm.items():\n",
        "            if cnorm == tnorm:\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "intake_targets = [\n",
        "    \"Open Conf Date\",\"Open Conf. Date\",\"Open Conference Date\",\n",
        "    \"Open ConfDate\",\"Open Conference Dt\",\"OpenConfDate\"\n",
        "]\n",
        "zip_targets = [\"GeoZip\",\"Geo Zip\",\"Site Zip\",\"Site  Zip\"]\n",
        "county_targets = [\"GeoCounty\",\"Geo County\",\"County\"]\n",
        "\n",
        "candidates = []\n",
        "for s in xls.sheet_names:\n",
        "    try:\n",
        "        df = pd.read_excel(CALOSHA_PATH, sheet_name=s)\n",
        "        ic = find_col(df, intake_targets)\n",
        "        zc = find_col(df, zip_targets)\n",
        "        cc = find_col(df, county_targets)\n",
        "        score = (ic is not None) + (zc is not None) + (cc is not None)\n",
        "        if score >= 2:  # needs intake + (zip or county)\n",
        "            candidates.append((s, ic, zc, cc, score, len(df.columns)))\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "# sort: best score first, then more columns (more likely the raw table)\n",
        "candidates.sort(key=lambda x: (-x[4], -x[5]))\n",
        "print(\"Candidates (best first):\")\n",
        "for s, ic, zc, cc, score, ncols in candidates[:8]:\n",
        "    print(f\"  • Sheet='{s}' | intake='{ic}' | zip='{zc}' | county='{cc}' | score={score} | ncols={ncols}\")\n",
        "\n",
        "if not candidates:\n",
        "    print(\"No suitable sheet found. Try 'Complaints', 'Sheet1', or 'Sheet3' manually.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aEZlnZC5cR7",
        "outputId": "7419775a-518a-4a0f-baca-6ac959fb4162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidates (best first):\n",
            "No suitable sheet found. Try 'Complaints', 'Sheet1', or 'Sheet3' manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "LIKELY_SHEETS = [\"Complaints\", \"Sheet1\", \"Sheet3\"]\n",
        "\n",
        "def find_col(df, targets):\n",
        "    norm = {c: c.strip().lower().replace(\".\", \"\").replace(\"_\", \"\").replace(\"  \",\" \")\n",
        "            for c in df.columns}\n",
        "    for t in targets:\n",
        "        tnorm = t.strip().lower().replace(\".\", \"\").replace(\"_\", \"\").replace(\"  \",\" \")\n",
        "        for c, cnorm in norm.items():\n",
        "            if cnorm == tnorm:\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "intake_candidates = [\"Open Conf Date\",\"Open Conf. Date\",\"Open Conference Date\",\"Open ConfDate\",\"Open Conference Dt\",\"OpenConfDate\"]\n",
        "zip_candidates    = [\"GeoZip\",\"Geo Zip\",\"Site Zip\",\"Site  Zip\"]\n",
        "county_candidates = [\"GeoCounty\",\"Geo County\",\"County\"]\n",
        "\n",
        "results = []\n",
        "for s in LIKELY_SHEETS:\n",
        "    try:\n",
        "        df = pd.read_excel(CALOSHA_PATH, sheet_name=s)\n",
        "        ic = find_col(df, intake_candidates)\n",
        "        zc = find_col(df, zip_candidates)\n",
        "        cc = find_col(df, county_candidates)\n",
        "        score = (ic is not None) + (zc is not None) + (cc is not None)\n",
        "        results.append((s, ic, zc, cc, score, len(df), len(df.columns)))\n",
        "    except Exception as e:\n",
        "        results.append((s, None, None, None, -1, 0, 0))\n",
        "\n",
        "print(\"Sheet scan (best first):\")\n",
        "for s, ic, zc, cc, score, nrows, ncols in sorted(results, key=lambda x: (-x[4], -x[5], -x[6])):\n",
        "    print(f\"  • {s:12s} | intake={ic} | zip={zc} | county={cc} | score={score} | rows={nrows} | cols={ncols}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ2wFVkB54CN",
        "outputId": "a129a889-9550-4b50-923a-12318cfabc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sheet scan (best first):\n",
            "  • Complaints   | intake=None | zip=None | county=None | score=0 | rows=3059 | cols=116\n",
            "  • Sheet3       | intake=None | zip=None | county=None | score=0 | rows=57 | cols=7\n",
            "  • Sheet1       | intake=None | zip=None | county=None | score=0 | rows=55 | cols=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, re\n",
        "\n",
        "SHEET = \"Complaints\"  # likely the raw table\n",
        "\n",
        "# patterns we expect to see somewhere in the header row\n",
        "PATTERNS = {\n",
        "    \"intake\":  re.compile(r\"open\\s*conf.*date\", re.I),\n",
        "    \"zip\":     re.compile(r\"\\bgeo\\s*zip\\b|\\bsite\\s*zip\\b\", re.I),\n",
        "    \"county\":  re.compile(r\"\\bgeo\\s*county\\b|\\bcounty\\b\", re.I),\n",
        "}\n",
        "\n",
        "# 1) read a small slice first without headers to find which row contains the header\n",
        "peek = pd.read_excel(CALOSHA_PATH, sheet_name=SHEET, header=None, nrows=30)\n",
        "header_row = None\n",
        "for i in range(len(peek)):\n",
        "    row_vals = [str(x) for x in peek.iloc[i].tolist()]\n",
        "    joined = \" | \".join(row_vals)\n",
        "    if any(p.search(joined) for p in PATTERNS.values()):\n",
        "        header_row = i\n",
        "        break\n",
        "\n",
        "print(\"Detected header row:\", header_row)\n",
        "\n",
        "# 2) re-read using that header row\n",
        "if header_row is None:\n",
        "    raise ValueError(\"Could not auto-detect a header row. Try increasing nrows or inspecting manually.\")\n",
        "\n",
        "df_calo = pd.read_excel(CALOSHA_PATH, sheet_name=SHEET, header=header_row)\n",
        "\n",
        "# 3) normalize headers (for matching & sanity)\n",
        "def norm(s):\n",
        "    s = str(s)\n",
        "    s = s.replace(\"\\xa0\",\" \")  # non-breaking spaces\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "df_calo.columns = [norm(c) for c in df_calo.columns]\n",
        "\n",
        "# 4) find the actual column names by regex matching on the cleaned headers\n",
        "def find_col_by_regex(cols, patt):\n",
        "    for c in cols:\n",
        "        if patt.search(c):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "intake_col = find_col_by_regex(df_calo.columns, PATTERNS[\"intake\"])\n",
        "zip_col    = find_col_by_regex(df_calo.columns, PATTERNS[\"zip\"])\n",
        "county_col = find_col_by_regex(df_calo.columns, PATTERNS[\"county\"])\n",
        "\n",
        "print(\"Resolved columns →\")\n",
        "print(\"  intake:\", intake_col)\n",
        "print(\"  zip   :\", zip_col)\n",
        "print(\"  county:\", county_col)\n",
        "\n",
        "# (optional) quick year counts to confirm we’ll retain rows\n",
        "years = pd.to_datetime(df_calo[intake_col], errors=\"coerce\").dt.year\n",
        "print(\"Counts by year:\\n\", years.value_counts().sort_index())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCCDbGdc6Zc9",
        "outputId": "b04f50fc-28b2-4764-9494-9e91f85a6d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected header row: 14\n",
            "Resolved columns →\n",
            "  intake: Open Conf Date\n",
            "  zip   : Site Zip\n",
            "  county: Site County\n",
            "Counts by year:\n",
            " Open Conf Date\n",
            "2015.0    273\n",
            "2016.0    266\n",
            "2017.0    315\n",
            "2018.0    293\n",
            "2019.0    315\n",
            "2020.0    133\n",
            "2021.0    246\n",
            "2022.0    330\n",
            "2023.0    144\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Read with header row 15 (0-indexed 14)\n",
        "raw = pd.read_excel(CALOSHA_PATH, sheet_name=\"Complaints\", header=14)\n",
        "\n",
        "# Normalize headers (collapse weird spaces/punctuation)\n",
        "raw.columns = (\n",
        "    raw.columns.astype(str)\n",
        "       .str.replace(\"\\xa0\",\" \", regex=False)\n",
        "       .str.replace(r\"\\s+\",\" \", regex=True)\n",
        "       .str.strip()\n",
        ")\n",
        "\n",
        "def pick_col(df, preferred, candidates):\n",
        "    # exact match first\n",
        "    if preferred and preferred in df.columns:\n",
        "        return preferred\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # relaxed: remove non-alnum for matching\n",
        "    def norm(s): return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n",
        "    normmap = {col: norm(col) for col in df.columns}\n",
        "    targets = ([preferred] if preferred else []) + list(candidates)\n",
        "    for t in targets:\n",
        "        key = norm(t)\n",
        "        for col, nm in normmap.items():\n",
        "            if nm == key:\n",
        "                return col\n",
        "    return None\n",
        "\n",
        "INTAKE = pick_col(raw, None, [\"Open Conf Date\",\"Open Conf. Date\",\"Open Conference Date\"])\n",
        "ZIPCOL = pick_col(raw, None, [\"GeoZip\",\"Geo Zip\",\"Site Zip\",\"Site  Zip\"])\n",
        "COUNTY = pick_col(raw, None, [\"GeoCounty\",\"Geo County\",\"Site County\",\"County\"])\n",
        "\n",
        "print(\"Resolved columns →\", \"intake:\", INTAKE, \"| zip:\", ZIPCOL, \"| county:\", COUNTY)\n",
        "\n",
        "if INTAKE is None:\n",
        "    raise ValueError(\"Could not locate an intake date column on 'Complaints' with header=14.\")\n",
        "\n",
        "dt = pd.to_datetime(raw[INTAKE], errors=\"coerce\")\n",
        "mask_years = dt.dt.year.isin(YEARS)\n",
        "cal_y = raw.loc[mask_years].copy()\n",
        "cal_y[\"__intake\"] = dt.loc[mask_years]\n",
        "\n",
        "# ZIP→5 and geo merge\n",
        "cal_y[\"ZIP\"] = cal_y[ZIPCOL].astype(str).apply(zip5) if ZIPCOL else \"\"\n",
        "merged = cal_y[[\"ZIP\"]].rename(columns={\"ZIP\":\"ZIP5\"}).merge(geo, how=\"left\", on=\"ZIP5\")\n",
        "merged.index = cal_y.index\n",
        "\n",
        "# County resolution (B and 1)\n",
        "county_from_agency = (cal_y[COUNTY] if COUNTY else \"\").fillna(\"\").astype(str).str.strip()\n",
        "county_from_zip = merged[\"GeoCounty\"].fillna(\"\").astype(str).str.strip()\n",
        "cal_y[\"County_resolved\"] = np.where(\n",
        "    (cal_y[\"ZIP\"] != \"\") & (county_from_zip != \"\"),\n",
        "    county_from_zip,\n",
        "    county_from_agency\n",
        ")\n",
        "\n",
        "# Rural flag + quick counts\n",
        "cal_y[\"Is_Rural_Worker\"] = cal_y[\"County_resolved\"].apply(lambda c: \"Y\" if c in RURAL_COUNTIES else \"N\")\n",
        "\n",
        "print(\"Cal/OSHA in YEARS:\", len(cal_y))\n",
        "print(\"\\nTop counties:\\n\", cal_y[\"County_resolved\"].value_counts().head(15))\n",
        "print(\"\\nRural flag counts:\\n\", cal_y[\"Is_Rural_Worker\"].value_counts(dropna=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_heXvfGSAuW_",
        "outputId": "668974e7-7b55-43dc-8f97-56e2b4bd6674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolved columns → intake: Open Conf Date | zip: GeoZip | county: GeoCounty\n",
            "Cal/OSHA in YEARS: 144\n",
            "\n",
            "Top counties:\n",
            " County_resolved\n",
            "Los Angeles       32\n",
            "Fresno            21\n",
            "San Diego         12\n",
            "Riverside          8\n",
            "Tulare             8\n",
            "Kern               8\n",
            "Sacramento         7\n",
            "Kings              5\n",
            "Imperial           5\n",
            "Contra Costa       5\n",
            "Madera             4\n",
            "Yolo               4\n",
            "San Bernardino     4\n",
            "Ventura            2\n",
            "Stanislaus         2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Rural flag counts:\n",
            " Is_Rural_Worker\n",
            "N    85\n",
            "Y    59\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final[\"Destination_Agency\"].eq(\"\").sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85iXeepndbks",
        "outputId": "3633982a-1dba-4e51-ce3f-a4004148092d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(0)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    }
  ]
}